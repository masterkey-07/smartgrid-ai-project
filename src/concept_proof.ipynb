{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predição do Estado de uma Smart Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domínio da Aplicação\n",
    "\n",
    "## Exploração da Base de Dados\n",
    "\n",
    "### Coletando os [dados simulados de estabilidade de uma rede elétrica](https://archive.ics.uci.edu/dataset/471/electrical+grid+stability+simulated+data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "electrical_grid_stability_simulated_data = fetch_ucirepo(id=471).data\n",
    "\n",
    "features = electrical_grid_stability_simulated_data.features \n",
    "targets = electrical_grid_stability_simulated_data.targets\n",
    "\n",
    "data = pd.merge(features, targets, left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicando os Atributos\n",
    "\n",
    "- tau[x]: Tempo de reação do participante (valor real no intervalo [0.5, 10] segundos). \n",
    "    - tau1 - o valor para o produtor de eletricidade;\n",
    "- p[x]: Potência nominal consumida (negativa) ou produzida (positiva) (valor real). \n",
    "    - Para consumidores, no intervalo [-0.5, -2] segundos^-2;\n",
    "    - p1 = abs(p2 + p3 + p4);\n",
    "- g[x]: Coeficiente (gamma) proporcional à elasticidade de preço (valor real no intervalo [0.05, 1] segundos^-1). \n",
    "    - g1 - o valor para o produtor de eletricidade;\n",
    "- stab: A parte real máxima da raiz da equação característica (se positiva - o sistema é linearmente instável) (valor real).\n",
    "- stabf: A classificação de estabilidade do sistema (categórica: estável/instável).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento dos Dados\n",
    "\n",
    "### Eliminação do dado discreto, `stabf`\n",
    "\n",
    "Com o objetivo de prever o próximo estado da rede, a partir do valor contínuo `stab`, temos que eliminar o valor `stabf` já que depende de `stab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['stabf'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise das Correlações\n",
    "\n",
    "Analisar as correlações das váriaveis para escolher as melhores váriaveis para os modelos de regressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Mapa de Calor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vendo a baixa correlação das variáveis $p_x$ com $stab$, é decidido a remoção desses valores, para melhorar o aprendizado do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['p1', 'p2', 'p3', 'p4'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecimento de Padrões e Aprendizados\n",
    "\n",
    "### Definição de Métodos e Classes de Apoio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def create_data_structure():\n",
    "    '''\n",
    "    Cria um objeto com a estrutura de dados preparada para armazenar as informações durante os treinamentos.\n",
    "    '''\n",
    "    return {\n",
    "        \"r2\": [],\n",
    "        \"mse\": [],\n",
    "        \"mae\": [],\n",
    "        \"pred\": [],\n",
    "        \"test\": [],\n",
    "        \"mape\": [],\n",
    "    }\n",
    "\n",
    "def get_class_name(object):\n",
    "    return object.__class__.__name__\n",
    "\n",
    "class Fitter:\n",
    "    '''\n",
    "    Classe de apoio para facilitar no treinamento de cada modelo.\n",
    "    '''\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) -> None:\n",
    "        self.__x_train = x_train\n",
    "        self.__y_train = y_train\n",
    "        self.__x_test = x_test\n",
    "        self.__y_test = y_test\n",
    "\n",
    "    def fit_model_and_measure(self, model, data):\n",
    "        model.fit(self.__x_train, self.__y_train)\n",
    "        \n",
    "        pred = model.predict(self.__x_test)\n",
    "\n",
    "        data['pred'].append(pred)\n",
    "        data['test'].append(self.__y_test)\n",
    "        data['r2'].append(r2_score(self.__y_test, pred))\n",
    "        data['mse'].append(mean_squared_error(self.__y_test, pred))\n",
    "        data['mae'].append(mean_absolute_error(self.__y_test, pred))\n",
    "        data['mape'].append(np.mean(np.abs((self.__y_test - pred) / self.__y_test)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição dos modelos a serem usados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, NuSVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge, ElasticNet, BayesianRidge, ARDRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "\n",
    "\n",
    "MODELS = [\n",
    "    BayesianRidge(),\n",
    "    ARDRegression(),\n",
    "    Lasso(alpha=0.1),\n",
    "    Ridge(alpha=1.0),\n",
    "    LinearRegression(),\n",
    "    SVR(kernel='linear', C=1.0),\n",
    "    NuSVR(nu=0.5, kernel='rbf'),\n",
    "    LGBMRegressor(max_depth=10, verbose=-1),\n",
    "    KNeighborsRegressor(n_neighbors=5),\n",
    "    ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    AdaBoostRegressor(n_estimators=100),\n",
    "    SGDRegressor(max_iter=1000, tol=1e-3),\n",
    "    ExtraTreesRegressor(n_estimators=100),\n",
    "    XGBRegressor(learning_rate=0.1, max_depth=8), \n",
    "    MLPRegressor(hidden_layer_sizes=(100,), max_iter=500),\n",
    "    RandomForestRegressor(n_estimators=100, random_state=0),\n",
    "    HistGradientBoostingRegressor(max_iter=100, learning_rate=0.1),\n",
    "    GradientBoostingRegressor(n_estimators=100, learning_rate=0.1),\n",
    "    DecisionTreeRegressor(max_depth=5, splitter='random', min_samples_leaf=5),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Validação dos Primeiros Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "outputs = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    outputs[get_class_name(model)] = create_data_structure()\n",
    " \n",
    "five_folds = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "STAB_COLUMN_INDEX = list(data.columns).index('stab')\n",
    "\n",
    "number_of_operations = len(MODELS) * 5 \n",
    "\n",
    "progress_bar = tqdm(total=number_of_operations)\n",
    "\n",
    "y_data = data['stab'].values\n",
    "x_data = data.drop(columns=['stab']).values\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(five_folds.split(data)):\n",
    "    y_train = y_data[train_index]\n",
    "    x_train = x_data[train_index]\n",
    "\n",
    "    y_test = y_data[test_index]\n",
    "    x_test = x_data[test_index]\n",
    "\n",
    "    fitter = Fitter(\n",
    "        x_train=x_train,\n",
    "        x_test=x_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "    )\n",
    "\n",
    "    for model in MODELS:\n",
    "        fitter.fit_model_and_measure(model, data=outputs[get_class_name(model)])\n",
    "        progress_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pós-Processamento e Conclusão\n",
    "\n",
    "## Definindo funções de apoio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics(error_structure):\n",
    "    metrics = {\n",
    "        'MSE': error_structure['mse'], \n",
    "        'MAE': error_structure['mae'], \n",
    "        'MAPE': error_structure['mape'],\n",
    "        'R2': error_structure['r2']\n",
    "    }\n",
    "\n",
    "    mean_std = {metric: (np.mean(scores), np.std(scores)) for metric, scores in metrics.items()}\n",
    "\n",
    "    return mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = ['MSE', 'MAE', 'MAPE', 'R2']\n",
    "\n",
    "model_metrics = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    model_metrics[get_class_name(model)] = create_metrics(outputs[get_class_name(model)])\n",
    "\n",
    "# Função para plotar gráficos de métricas\n",
    "def plot_metrics(metric_name, model_metrics:dict, reverse=False):\n",
    "    width = 0.2\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "    sorted_metrics = sorted(model_metrics.items(), key=lambda x:x[1][metric_name][0], reverse=reverse)\n",
    "\n",
    "    for index, (model_name, metric_values) in enumerate(sorted_metrics):\n",
    "        x_position = index * width\n",
    "\n",
    "        height = metric_values[metric_name][0] \n",
    "\n",
    "        y_error = metric_values[metric_name][1]\n",
    "\n",
    "        ax.bar(x_position, height, width, label=model_name, yerr=y_error)\n",
    "\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title(f'{metric_name} Scores by Model')\n",
    "    ax.set_xticklabels(\"\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for metric in METRICS:\n",
    "    plot_metrics(metric, model_metrics, metric == 'R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtragem dos Modelos\n",
    "\n",
    "A partir dessa primeira comparação, será escolhido os modelos\n",
    "- NuSVR\n",
    "- LGBMRegressor\n",
    "- HistGradientBoostRegressor\n",
    "- XGBoost\n",
    "- ExtraTreeRegressor\n",
    "\n",
    "E para modelo, aplicar o método de GridSearch para otimizar o modelo sobre o problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODELS = [\n",
    "    (NuSVR(), {\n",
    "        'nu': [0.1, 0.5, 0.9],\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'coef0': [0.1, 0.5, 0.9],\n",
    "        'shrinking': [True, False]\n",
    "    }),\n",
    "    (XGBRegressor(), {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.6, 0.8, 1.0]\n",
    "    }), \n",
    "    (LGBMRegressor(), {\n",
    "        'boosting_type': ['gbdt', 'dart', 'rf'],\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'verbosity': [-1]\n",
    "    })\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import cpu_count\n",
    "from IPython.display import clear_output\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, train_test_split\n",
    "\n",
    "number_of_cpus = cpu_count()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)\n",
    "\n",
    "y_data = data['stab'].values\n",
    "x_data = data.drop(columns=['stab']).values\n",
    "\n",
    "fitter = Fitter(\n",
    "    x_train=x_train,\n",
    "    x_test=x_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "best_outputs = {}\n",
    "\n",
    "for model, _ in BEST_MODELS:\n",
    "    best_outputs[get_class_name(model)] = create_data_structure()\n",
    "\n",
    "for model, params in BEST_MODELS:\n",
    "    model_name = get_class_name(model)\n",
    "\n",
    "    grid_search = HalvingGridSearchCV(model, params, n_jobs=number_of_cpus, cv=2, scoring='r2') #['r2', 'neg_median_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error']\n",
    "\n",
    "    fitter.fit_model_and_measure(grid_search, data=best_outputs[get_class_name(model)])\n",
    "\n",
    "    best_outputs[model_name]['best_model'] = grid_search.best_params_\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = ['MSE', 'MAE', 'MAPE', 'R2']\n",
    "\n",
    "model_metrics = {}\n",
    "\n",
    "for model,_ in BEST_MODELS:\n",
    "    print(model)\n",
    "    print(best_outputs[get_class_name(model)]['best_model'])\n",
    "    model_metrics[get_class_name(model)] = create_metrics(best_outputs[get_class_name(model)])\n",
    "\n",
    "# Função para plotar gráficos de métricas\n",
    "def plot_metrics(metric_name, model_metrics:dict, reverse=False):\n",
    "    width = 0.2\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "    sorted_metrics = sorted(model_metrics.items(), key=lambda x:x[1][metric_name][0], reverse=reverse)\n",
    "\n",
    "    for index, (model_name, metric_values) in enumerate(sorted_metrics):\n",
    "        x_position = index * width\n",
    "\n",
    "        height = metric_values[metric_name][0] \n",
    "\n",
    "        y_error = metric_values[metric_name][1]\n",
    "\n",
    "        ax.bar(x_position, height, width, label=model_name, yerr=y_error)\n",
    "\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title(f'{metric_name} Scores by Model')\n",
    "    ax.set_xticklabels(\"\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for metric in METRICS:\n",
    "    plot_metrics(metric, model_metrics, metric == 'R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "fitter = Fitter(\n",
    "    x_train=x_train,\n",
    "    x_test=x_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'nu': [0.9],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(NuSVR(), params, n_jobs=number_of_cpus, cv=5, scoring='r2', verbose=1)\n",
    "\n",
    "results = create_data_structure()\n",
    "\n",
    "fitter.fit_model_and_measure(grid_search, results)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "Responder à pergunta: \"Você acredita que será possível entregar tudo que prometeu no documento da Proposta?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sim, por mais que esteja apenas no começo do projeto, já é possivel fazer algumas análises utilizando regressão linear e arvore de regressão como visto em aula, com o que já temos fica facil crial pipelines para os ajustes nos dados e também abre a possibilidade para random forests. Com o objetivo final de evitar overfitting e garantir também que seja possivel obter respostas e tomadas de decisões razoáveis em relação aos dados estabelecidos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
